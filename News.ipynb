{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "200d4d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링하고 싶은 뉴스 검색어를 입력해주세요: 데이터분석\n",
      "크롤링하고 싶은 뉴스의 시작 날짜를 입력해주세요(ex:2024.01.01): 2023.12.17\n",
      "크롤링하고 싶은 뉴스의 종료 날짜를 입력해주세요(ex:2024.01.01): 2024.01.15\n",
      "크롤링하고 싶은 뉴스의 정렬방법을 입력해주세요(관련도순 = 0  최신순 = 1  오래된순 = 2): 0\n",
      "크롤링하고 싶은 뉴스의 페이지 수를 입력해주세요: 10\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3 \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "from html import escape\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def news_crawl(query, start_date, end_date, sort_type, max_page):\n",
    "    dbpath = \"news_info.db\" \n",
    "    conn = sqlite3.connect(dbpath)\n",
    "    cur = conn.cursor() \n",
    "\n",
    "    script = \"\"\"\n",
    "    DROP TABLE IF EXISTS news_crawl;\n",
    "\n",
    "    CREATE TABLE news_crawl(\n",
    "      id INTEGER PRIMARY KEY AUTOINCREMENT,  -- 뉴스의 ID 값\n",
    "      date TEXT,                             -- 뉴스의 작성일\n",
    "      title TEXT,                            -- 뉴스의 제목\n",
    "      summary TEXT,                          -- 뉴스의 요약\n",
    "      link TEXT,                             -- 뉴스의 원문 링크\n",
    "      detailed_link TEXT,                    -- 뉴스의 상세 페이지 링크\n",
    "      content TEXT,                          -- 뉴스의 본문 내용\n",
    "      press TEXT                             -- 뉴스의 언론사\n",
    "    );\n",
    "    \"\"\"\n",
    "    cur.executescript(script)\n",
    "        \n",
    "    if query == '':\n",
    "        query = '데이터 분석'\n",
    "    if len(start_date) != 10:\n",
    "        start_date = '2024.01.01'\n",
    "    if len(end_date) != 10:\n",
    "        end_date = '2024.01.11'\n",
    "    if sort_type not in ['0', '1', '2']:\n",
    "        sort_type = '0'\n",
    "    start_date = start_date.replace(\".\", \"\")\n",
    "    end_date = end_date.replace(\".\", \"\")\n",
    "    start_page=1\n",
    "\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "\n",
    "    for page in range(1, max_page + 1):\n",
    "        current_call = 1 + (page - 1) * 10\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query \\\n",
    "              + \"&sort=\" + sort_type \\\n",
    "              + \"&ds=\" + start_date \\\n",
    "              + \"&de=\" + end_date \\\n",
    "              + \"&start=\" + str(current_call)\n",
    "\n",
    "        web = requests.get(url, headers=headers).content\n",
    "        source = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "        # 각 페이지 내의 모든 뉴스 기사를 순회합니다.\n",
    "        for article in source.find_all('div', {'class': 'news_area'}):\n",
    "            title = article.find('a', {'class': 'news_tit'}).get('title').replace(\"'\", \"''\")\n",
    "            link = article.find('a', {'class': 'news_tit'}).get('href').replace(\"'\", \"''\")\n",
    "            summary = article.find('a', {'class': 'api_txt_lines dsc_txt_wrap'}).get_text().replace(\"'\", \"''\")\n",
    "            detailed_url = \"\"\n",
    "            #네이버 뉴스에 등록된 뉴스만 사용\n",
    "            for urls in article.find_all('a', {'class': 'info'}):\n",
    "                if urls[\"href\"].startswith(\"https://n.news.naver.com\"):\n",
    "                    detailed_url = urls[\"href\"].replace(\"'\", \"''\")\n",
    "                    break\n",
    "            if detailed_url:\n",
    "                response = requests.get(detailed_url, headers=headers)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                press_company = soup.find('em', {'class':'media_end_linked_more_point'}).get_text()\n",
    "                date = soup.find('span', {'class' : 'media_end_head_info_datestamp_time'}).get_text()\n",
    "                content_area = soup.find(id='dic_area')\n",
    "                news_content = content_area.get_text(strip=True).replace(\"'\", \"''\") if content_area else \"본문을 찾을 수 없습니다.\"\n",
    "                news_content = escape(news_content)\n",
    "\n",
    "                base_sql = \"INSERT INTO news_crawl(date, title, summary, link, detailed_link, content, press) values('{}','{}', '{}', '{}', '{}', '{}','{}')\"\n",
    "                sql_query = base_sql.format(date, title, summary, link, detailed_url, news_content, press_company)\n",
    "                cur.execute(sql_query)\n",
    "                conn.commit()\n",
    "            \n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# 사용자 입력을 통한 크롤링 실행\n",
    "query = input('크롤링하고 싶은 뉴스 검색어를 입력해주세요: ')\n",
    "start_date = input('크롤링하고 싶은 뉴스의 시작 날짜를 입력해주세요(ex:2024.01.01): ')\n",
    "end_date = input('크롤링하고 싶은 뉴스의 종료 날짜를 입력해주세요(ex:2024.01.01): ')\n",
    "sort_type = int(input('크롤링하고 싶은 뉴스의 정렬방법을 입력해주세요(관련도순 = 0  최신순 = 1  오래된순 = 2): '))\n",
    "max_page = int(input('크롤링하고 싶은 뉴스의 페이지 수를 입력해주세요: '))\n",
    "news_crawl(query, start_date, end_date, sort_type, max_page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d636606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 저장\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# 데이터베이스에서 데이터 읽기\n",
    "conn = sqlite3.connect('news_info.db')\n",
    "query = \"SELECT * FROM news_crawl\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# 데이터를 CSV 파일로 변환\n",
    "csv_file_path = \"news_data.csv\"\n",
    "df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dedc65e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [15/Jan/2024 16:16:18] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Jan/2024 16:16:18] \"GET /static/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [15/Jan/2024 16:17:01] \"POST /process HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Jan/2024 16:17:40] \"GET /static/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [15/Jan/2024 16:17:48] \"POST /process HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Jan/2024 16:18:37] \"POST /process HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Jan/2024 16:19:56] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#html + gpt api\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import sqlite3\n",
    "import html\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# API 키를 파일에서 읽기\n",
    "with open('OPENAI_API_KEY.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "    \n",
    "client = OpenAI(api_key=api_key)\n",
    "def generate_gpt_content(news_content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"'{news_content}' 위 내용을 바탕으로 한국어로 개선된 뉴스본문을 작성해. 그리고 50자 이내의 요약과 30자 이내의 제목도 생성해. 반드시 뉴스본문 앞에는 gpt_content: 를 요약 앞에는 gpt_summary: 를, 제목 앞에는 gpt_title: 을 붙여. 그리고 제목, 요약, 본문 순으로 나에게 보여줘.\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "@app.route('/')\n",
    "def show_news():\n",
    "    # 데이터베이스에서 뉴스 데이터를 가져옵니다.\n",
    "    conn = sqlite3.connect('news_info.db')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM news_crawl\")\n",
    "    news_data = cur.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    decoded_news_data = []\n",
    "    for news in news_data:\n",
    "        decoded_content = news[6]\n",
    "        decoded_news = list(news)\n",
    "        decoded_news[6] = decoded_content\n",
    "        decoded_news_data.append(decoded_news)\n",
    "        # HTML 템플릿에 데이터를 전달합니다.\n",
    "    return render_template('news_template.html', news_data=decoded_news_data)\n",
    "@app.route('/process', methods=['POST'])\n",
    "\n",
    "def process():\n",
    "    news_content = request.form['content']\n",
    "    decoded_content = html.unescape(news_content)\n",
    "    app.logger.info('Received content: %s', decoded_content)\n",
    "\n",
    "    gpt_response = generate_gpt_content(decoded_content)\n",
    "\n",
    "    return jsonify({'gpt_result': gpt_response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795db86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
